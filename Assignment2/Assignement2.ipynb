{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second Home Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Alunos:**\n",
    "    Pedro Oliveira (52754), Rodrigo Ferreira (51032), Rui Roque (57588)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1. Importação das bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, KFold\n",
    "from sklearn.metrics import explained_variance_score\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.2. Importação dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>number_of_elements</th>\n",
       "      <th>mean_atomic_mass</th>\n",
       "      <th>wtd_mean_atomic_mass</th>\n",
       "      <th>gmean_atomic_mass</th>\n",
       "      <th>wtd_gmean_atomic_mass</th>\n",
       "      <th>entropy_atomic_mass</th>\n",
       "      <th>wtd_entropy_atomic_mass</th>\n",
       "      <th>range_atomic_mass</th>\n",
       "      <th>wtd_range_atomic_mass</th>\n",
       "      <th>std_atomic_mass</th>\n",
       "      <th>wtd_std_atomic_mass</th>\n",
       "      <th>mean_fie</th>\n",
       "      <th>wtd_mean_fie</th>\n",
       "      <th>gmean_fie</th>\n",
       "      <th>wtd_gmean_fie</th>\n",
       "      <th>entropy_fie</th>\n",
       "      <th>wtd_entropy_fie</th>\n",
       "      <th>range_fie</th>\n",
       "      <th>wtd_range_fie</th>\n",
       "      <th>std_fie</th>\n",
       "      <th>wtd_std_fie</th>\n",
       "      <th>mean_atomic_radius</th>\n",
       "      <th>wtd_mean_atomic_radius</th>\n",
       "      <th>gmean_atomic_radius</th>\n",
       "      <th>wtd_gmean_atomic_radius</th>\n",
       "      <th>entropy_atomic_radius</th>\n",
       "      <th>wtd_entropy_atomic_radius</th>\n",
       "      <th>range_atomic_radius</th>\n",
       "      <th>wtd_range_atomic_radius</th>\n",
       "      <th>std_atomic_radius</th>\n",
       "      <th>wtd_std_atomic_radius</th>\n",
       "      <th>mean_Density</th>\n",
       "      <th>wtd_mean_Density</th>\n",
       "      <th>gmean_Density</th>\n",
       "      <th>wtd_gmean_Density</th>\n",
       "      <th>entropy_Density</th>\n",
       "      <th>wtd_entropy_Density</th>\n",
       "      <th>range_Density</th>\n",
       "      <th>wtd_range_Density</th>\n",
       "      <th>std_Density</th>\n",
       "      <th>wtd_std_Density</th>\n",
       "      <th>mean_ElectronAffinity</th>\n",
       "      <th>wtd_mean_ElectronAffinity</th>\n",
       "      <th>gmean_ElectronAffinity</th>\n",
       "      <th>wtd_gmean_ElectronAffinity</th>\n",
       "      <th>entropy_ElectronAffinity</th>\n",
       "      <th>wtd_entropy_ElectronAffinity</th>\n",
       "      <th>range_ElectronAffinity</th>\n",
       "      <th>wtd_range_ElectronAffinity</th>\n",
       "      <th>std_ElectronAffinity</th>\n",
       "      <th>wtd_std_ElectronAffinity</th>\n",
       "      <th>mean_FusionHeat</th>\n",
       "      <th>wtd_mean_FusionHeat</th>\n",
       "      <th>gmean_FusionHeat</th>\n",
       "      <th>wtd_gmean_FusionHeat</th>\n",
       "      <th>entropy_FusionHeat</th>\n",
       "      <th>wtd_entropy_FusionHeat</th>\n",
       "      <th>range_FusionHeat</th>\n",
       "      <th>wtd_range_FusionHeat</th>\n",
       "      <th>std_FusionHeat</th>\n",
       "      <th>wtd_std_FusionHeat</th>\n",
       "      <th>mean_ThermalConductivity</th>\n",
       "      <th>wtd_mean_ThermalConductivity</th>\n",
       "      <th>gmean_ThermalConductivity</th>\n",
       "      <th>wtd_gmean_ThermalConductivity</th>\n",
       "      <th>entropy_ThermalConductivity</th>\n",
       "      <th>wtd_entropy_ThermalConductivity</th>\n",
       "      <th>range_ThermalConductivity</th>\n",
       "      <th>wtd_range_ThermalConductivity</th>\n",
       "      <th>std_ThermalConductivity</th>\n",
       "      <th>wtd_std_ThermalConductivity</th>\n",
       "      <th>mean_Valence</th>\n",
       "      <th>wtd_mean_Valence</th>\n",
       "      <th>gmean_Valence</th>\n",
       "      <th>wtd_gmean_Valence</th>\n",
       "      <th>entropy_Valence</th>\n",
       "      <th>wtd_entropy_Valence</th>\n",
       "      <th>range_Valence</th>\n",
       "      <th>wtd_range_Valence</th>\n",
       "      <th>std_Valence</th>\n",
       "      <th>wtd_std_Valence</th>\n",
       "      <th>critical_temp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>88.944468</td>\n",
       "      <td>57.862692</td>\n",
       "      <td>66.361592</td>\n",
       "      <td>36.116612</td>\n",
       "      <td>1.181795</td>\n",
       "      <td>1.062396</td>\n",
       "      <td>122.90607</td>\n",
       "      <td>31.794921</td>\n",
       "      <td>51.968828</td>\n",
       "      <td>53.622535</td>\n",
       "      <td>775.425</td>\n",
       "      <td>1010.268571</td>\n",
       "      <td>718.152900</td>\n",
       "      <td>938.016780</td>\n",
       "      <td>1.305967</td>\n",
       "      <td>0.791488</td>\n",
       "      <td>810.6</td>\n",
       "      <td>735.985714</td>\n",
       "      <td>323.811808</td>\n",
       "      <td>355.562967</td>\n",
       "      <td>160.25</td>\n",
       "      <td>105.514286</td>\n",
       "      <td>136.126003</td>\n",
       "      <td>84.528423</td>\n",
       "      <td>1.259244</td>\n",
       "      <td>1.207040</td>\n",
       "      <td>205</td>\n",
       "      <td>42.914286</td>\n",
       "      <td>75.237540</td>\n",
       "      <td>69.235569</td>\n",
       "      <td>4654.35725</td>\n",
       "      <td>2961.502286</td>\n",
       "      <td>724.953211</td>\n",
       "      <td>53.543811</td>\n",
       "      <td>1.033129</td>\n",
       "      <td>0.814598</td>\n",
       "      <td>8958.571</td>\n",
       "      <td>1579.583429</td>\n",
       "      <td>3306.162897</td>\n",
       "      <td>3572.596624</td>\n",
       "      <td>81.8375</td>\n",
       "      <td>111.727143</td>\n",
       "      <td>60.123179</td>\n",
       "      <td>99.414682</td>\n",
       "      <td>1.159687</td>\n",
       "      <td>0.787382</td>\n",
       "      <td>127.05</td>\n",
       "      <td>80.987143</td>\n",
       "      <td>51.433712</td>\n",
       "      <td>42.558396</td>\n",
       "      <td>6.9055</td>\n",
       "      <td>3.846857</td>\n",
       "      <td>3.479475</td>\n",
       "      <td>1.040986</td>\n",
       "      <td>1.088575</td>\n",
       "      <td>0.994998</td>\n",
       "      <td>12.878</td>\n",
       "      <td>1.744571</td>\n",
       "      <td>4.599064</td>\n",
       "      <td>4.666920</td>\n",
       "      <td>107.756645</td>\n",
       "      <td>61.015189</td>\n",
       "      <td>7.062488</td>\n",
       "      <td>0.621979</td>\n",
       "      <td>0.308148</td>\n",
       "      <td>0.262848</td>\n",
       "      <td>399.97342</td>\n",
       "      <td>57.127669</td>\n",
       "      <td>168.854244</td>\n",
       "      <td>138.517163</td>\n",
       "      <td>2.25</td>\n",
       "      <td>2.257143</td>\n",
       "      <td>2.213364</td>\n",
       "      <td>2.219783</td>\n",
       "      <td>1.368922</td>\n",
       "      <td>1.066221</td>\n",
       "      <td>1</td>\n",
       "      <td>1.085714</td>\n",
       "      <td>0.433013</td>\n",
       "      <td>0.437059</td>\n",
       "      <td>29.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>92.729214</td>\n",
       "      <td>58.518416</td>\n",
       "      <td>73.132787</td>\n",
       "      <td>36.396602</td>\n",
       "      <td>1.449309</td>\n",
       "      <td>1.057755</td>\n",
       "      <td>122.90607</td>\n",
       "      <td>36.161939</td>\n",
       "      <td>47.094633</td>\n",
       "      <td>53.979870</td>\n",
       "      <td>766.440</td>\n",
       "      <td>1010.612857</td>\n",
       "      <td>720.605511</td>\n",
       "      <td>938.745413</td>\n",
       "      <td>1.544145</td>\n",
       "      <td>0.807078</td>\n",
       "      <td>810.6</td>\n",
       "      <td>743.164286</td>\n",
       "      <td>290.183029</td>\n",
       "      <td>354.963511</td>\n",
       "      <td>161.20</td>\n",
       "      <td>104.971429</td>\n",
       "      <td>141.465215</td>\n",
       "      <td>84.370167</td>\n",
       "      <td>1.508328</td>\n",
       "      <td>1.204115</td>\n",
       "      <td>205</td>\n",
       "      <td>50.571429</td>\n",
       "      <td>67.321319</td>\n",
       "      <td>68.008817</td>\n",
       "      <td>5821.48580</td>\n",
       "      <td>3021.016571</td>\n",
       "      <td>1237.095080</td>\n",
       "      <td>54.095718</td>\n",
       "      <td>1.314442</td>\n",
       "      <td>0.914802</td>\n",
       "      <td>10488.571</td>\n",
       "      <td>1667.383429</td>\n",
       "      <td>3767.403176</td>\n",
       "      <td>3632.649185</td>\n",
       "      <td>90.8900</td>\n",
       "      <td>112.316429</td>\n",
       "      <td>69.833315</td>\n",
       "      <td>101.166398</td>\n",
       "      <td>1.427997</td>\n",
       "      <td>0.838666</td>\n",
       "      <td>127.05</td>\n",
       "      <td>81.207857</td>\n",
       "      <td>49.438167</td>\n",
       "      <td>41.667621</td>\n",
       "      <td>7.7844</td>\n",
       "      <td>3.796857</td>\n",
       "      <td>4.403790</td>\n",
       "      <td>1.035251</td>\n",
       "      <td>1.374977</td>\n",
       "      <td>1.073094</td>\n",
       "      <td>12.878</td>\n",
       "      <td>1.595714</td>\n",
       "      <td>4.473363</td>\n",
       "      <td>4.603000</td>\n",
       "      <td>172.205316</td>\n",
       "      <td>61.372331</td>\n",
       "      <td>16.064228</td>\n",
       "      <td>0.619735</td>\n",
       "      <td>0.847404</td>\n",
       "      <td>0.567706</td>\n",
       "      <td>429.97342</td>\n",
       "      <td>51.413383</td>\n",
       "      <td>198.554600</td>\n",
       "      <td>139.630922</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.257143</td>\n",
       "      <td>1.888175</td>\n",
       "      <td>2.210679</td>\n",
       "      <td>1.557113</td>\n",
       "      <td>1.047221</td>\n",
       "      <td>2</td>\n",
       "      <td>1.128571</td>\n",
       "      <td>0.632456</td>\n",
       "      <td>0.468606</td>\n",
       "      <td>26.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>88.944468</td>\n",
       "      <td>57.885242</td>\n",
       "      <td>66.361592</td>\n",
       "      <td>36.122509</td>\n",
       "      <td>1.181795</td>\n",
       "      <td>0.975980</td>\n",
       "      <td>122.90607</td>\n",
       "      <td>35.741099</td>\n",
       "      <td>51.968828</td>\n",
       "      <td>53.656268</td>\n",
       "      <td>775.425</td>\n",
       "      <td>1010.820000</td>\n",
       "      <td>718.152900</td>\n",
       "      <td>939.009036</td>\n",
       "      <td>1.305967</td>\n",
       "      <td>0.773620</td>\n",
       "      <td>810.6</td>\n",
       "      <td>743.164286</td>\n",
       "      <td>323.811808</td>\n",
       "      <td>354.804183</td>\n",
       "      <td>160.25</td>\n",
       "      <td>104.685714</td>\n",
       "      <td>136.126003</td>\n",
       "      <td>84.214573</td>\n",
       "      <td>1.259244</td>\n",
       "      <td>1.132547</td>\n",
       "      <td>205</td>\n",
       "      <td>49.314286</td>\n",
       "      <td>75.237540</td>\n",
       "      <td>67.797712</td>\n",
       "      <td>4654.35725</td>\n",
       "      <td>2999.159429</td>\n",
       "      <td>724.953211</td>\n",
       "      <td>53.974022</td>\n",
       "      <td>1.033129</td>\n",
       "      <td>0.760305</td>\n",
       "      <td>8958.571</td>\n",
       "      <td>1667.383429</td>\n",
       "      <td>3306.162897</td>\n",
       "      <td>3592.019281</td>\n",
       "      <td>81.8375</td>\n",
       "      <td>112.213571</td>\n",
       "      <td>60.123179</td>\n",
       "      <td>101.082152</td>\n",
       "      <td>1.159687</td>\n",
       "      <td>0.786007</td>\n",
       "      <td>127.05</td>\n",
       "      <td>81.207857</td>\n",
       "      <td>51.433712</td>\n",
       "      <td>41.639878</td>\n",
       "      <td>6.9055</td>\n",
       "      <td>3.822571</td>\n",
       "      <td>3.479475</td>\n",
       "      <td>1.037439</td>\n",
       "      <td>1.088575</td>\n",
       "      <td>0.927479</td>\n",
       "      <td>12.878</td>\n",
       "      <td>1.757143</td>\n",
       "      <td>4.599064</td>\n",
       "      <td>4.649635</td>\n",
       "      <td>107.756645</td>\n",
       "      <td>60.943760</td>\n",
       "      <td>7.062488</td>\n",
       "      <td>0.619095</td>\n",
       "      <td>0.308148</td>\n",
       "      <td>0.250477</td>\n",
       "      <td>399.97342</td>\n",
       "      <td>57.127669</td>\n",
       "      <td>168.854244</td>\n",
       "      <td>138.540613</td>\n",
       "      <td>2.25</td>\n",
       "      <td>2.271429</td>\n",
       "      <td>2.213364</td>\n",
       "      <td>2.232679</td>\n",
       "      <td>1.368922</td>\n",
       "      <td>1.029175</td>\n",
       "      <td>1</td>\n",
       "      <td>1.114286</td>\n",
       "      <td>0.433013</td>\n",
       "      <td>0.444697</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>88.944468</td>\n",
       "      <td>57.873967</td>\n",
       "      <td>66.361592</td>\n",
       "      <td>36.119560</td>\n",
       "      <td>1.181795</td>\n",
       "      <td>1.022291</td>\n",
       "      <td>122.90607</td>\n",
       "      <td>33.768010</td>\n",
       "      <td>51.968828</td>\n",
       "      <td>53.639405</td>\n",
       "      <td>775.425</td>\n",
       "      <td>1010.544286</td>\n",
       "      <td>718.152900</td>\n",
       "      <td>938.512777</td>\n",
       "      <td>1.305967</td>\n",
       "      <td>0.783207</td>\n",
       "      <td>810.6</td>\n",
       "      <td>739.575000</td>\n",
       "      <td>323.811808</td>\n",
       "      <td>355.183884</td>\n",
       "      <td>160.25</td>\n",
       "      <td>105.100000</td>\n",
       "      <td>136.126003</td>\n",
       "      <td>84.371352</td>\n",
       "      <td>1.259244</td>\n",
       "      <td>1.173033</td>\n",
       "      <td>205</td>\n",
       "      <td>46.114286</td>\n",
       "      <td>75.237540</td>\n",
       "      <td>68.521665</td>\n",
       "      <td>4654.35725</td>\n",
       "      <td>2980.330857</td>\n",
       "      <td>724.953211</td>\n",
       "      <td>53.758486</td>\n",
       "      <td>1.033129</td>\n",
       "      <td>0.788889</td>\n",
       "      <td>8958.571</td>\n",
       "      <td>1623.483429</td>\n",
       "      <td>3306.162897</td>\n",
       "      <td>3582.370597</td>\n",
       "      <td>81.8375</td>\n",
       "      <td>111.970357</td>\n",
       "      <td>60.123179</td>\n",
       "      <td>100.244950</td>\n",
       "      <td>1.159687</td>\n",
       "      <td>0.786900</td>\n",
       "      <td>127.05</td>\n",
       "      <td>81.097500</td>\n",
       "      <td>51.433712</td>\n",
       "      <td>42.102344</td>\n",
       "      <td>6.9055</td>\n",
       "      <td>3.834714</td>\n",
       "      <td>3.479475</td>\n",
       "      <td>1.039211</td>\n",
       "      <td>1.088575</td>\n",
       "      <td>0.964031</td>\n",
       "      <td>12.878</td>\n",
       "      <td>1.744571</td>\n",
       "      <td>4.599064</td>\n",
       "      <td>4.658301</td>\n",
       "      <td>107.756645</td>\n",
       "      <td>60.979474</td>\n",
       "      <td>7.062488</td>\n",
       "      <td>0.620535</td>\n",
       "      <td>0.308148</td>\n",
       "      <td>0.257045</td>\n",
       "      <td>399.97342</td>\n",
       "      <td>57.127669</td>\n",
       "      <td>168.854244</td>\n",
       "      <td>138.528893</td>\n",
       "      <td>2.25</td>\n",
       "      <td>2.264286</td>\n",
       "      <td>2.213364</td>\n",
       "      <td>2.226222</td>\n",
       "      <td>1.368922</td>\n",
       "      <td>1.048834</td>\n",
       "      <td>1</td>\n",
       "      <td>1.100000</td>\n",
       "      <td>0.433013</td>\n",
       "      <td>0.440952</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>88.944468</td>\n",
       "      <td>57.840143</td>\n",
       "      <td>66.361592</td>\n",
       "      <td>36.110716</td>\n",
       "      <td>1.181795</td>\n",
       "      <td>1.129224</td>\n",
       "      <td>122.90607</td>\n",
       "      <td>27.848743</td>\n",
       "      <td>51.968828</td>\n",
       "      <td>53.588771</td>\n",
       "      <td>775.425</td>\n",
       "      <td>1009.717143</td>\n",
       "      <td>718.152900</td>\n",
       "      <td>937.025573</td>\n",
       "      <td>1.305967</td>\n",
       "      <td>0.805230</td>\n",
       "      <td>810.6</td>\n",
       "      <td>728.807143</td>\n",
       "      <td>323.811808</td>\n",
       "      <td>356.319281</td>\n",
       "      <td>160.25</td>\n",
       "      <td>106.342857</td>\n",
       "      <td>136.126003</td>\n",
       "      <td>84.843442</td>\n",
       "      <td>1.259244</td>\n",
       "      <td>1.261194</td>\n",
       "      <td>205</td>\n",
       "      <td>36.514286</td>\n",
       "      <td>75.237540</td>\n",
       "      <td>70.634448</td>\n",
       "      <td>4654.35725</td>\n",
       "      <td>2923.845143</td>\n",
       "      <td>724.953211</td>\n",
       "      <td>53.117029</td>\n",
       "      <td>1.033129</td>\n",
       "      <td>0.859811</td>\n",
       "      <td>8958.571</td>\n",
       "      <td>1491.783429</td>\n",
       "      <td>3306.162897</td>\n",
       "      <td>3552.668664</td>\n",
       "      <td>81.8375</td>\n",
       "      <td>111.240714</td>\n",
       "      <td>60.123179</td>\n",
       "      <td>97.774719</td>\n",
       "      <td>1.159687</td>\n",
       "      <td>0.787396</td>\n",
       "      <td>127.05</td>\n",
       "      <td>80.766429</td>\n",
       "      <td>51.433712</td>\n",
       "      <td>43.452059</td>\n",
       "      <td>6.9055</td>\n",
       "      <td>3.871143</td>\n",
       "      <td>3.479475</td>\n",
       "      <td>1.044545</td>\n",
       "      <td>1.088575</td>\n",
       "      <td>1.044970</td>\n",
       "      <td>12.878</td>\n",
       "      <td>1.744571</td>\n",
       "      <td>4.599064</td>\n",
       "      <td>4.684014</td>\n",
       "      <td>107.756645</td>\n",
       "      <td>61.086617</td>\n",
       "      <td>7.062488</td>\n",
       "      <td>0.624878</td>\n",
       "      <td>0.308148</td>\n",
       "      <td>0.272820</td>\n",
       "      <td>399.97342</td>\n",
       "      <td>57.127669</td>\n",
       "      <td>168.854244</td>\n",
       "      <td>138.493671</td>\n",
       "      <td>2.25</td>\n",
       "      <td>2.242857</td>\n",
       "      <td>2.213364</td>\n",
       "      <td>2.206963</td>\n",
       "      <td>1.368922</td>\n",
       "      <td>1.096052</td>\n",
       "      <td>1</td>\n",
       "      <td>1.057143</td>\n",
       "      <td>0.433013</td>\n",
       "      <td>0.428809</td>\n",
       "      <td>23.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   number_of_elements  mean_atomic_mass  wtd_mean_atomic_mass  gmean_atomic_mass  wtd_gmean_atomic_mass  entropy_atomic_mass  wtd_entropy_atomic_mass  range_atomic_mass  wtd_range_atomic_mass  std_atomic_mass  wtd_std_atomic_mass  mean_fie  wtd_mean_fie   gmean_fie  wtd_gmean_fie  entropy_fie  wtd_entropy_fie  range_fie  wtd_range_fie     std_fie  wtd_std_fie  mean_atomic_radius  wtd_mean_atomic_radius  gmean_atomic_radius  wtd_gmean_atomic_radius  entropy_atomic_radius  wtd_entropy_atomic_radius  range_atomic_radius  wtd_range_atomic_radius  std_atomic_radius  wtd_std_atomic_radius  mean_Density  wtd_mean_Density  gmean_Density  wtd_gmean_Density  entropy_Density  wtd_entropy_Density  range_Density  wtd_range_Density  std_Density  wtd_std_Density  mean_ElectronAffinity  wtd_mean_ElectronAffinity  gmean_ElectronAffinity  wtd_gmean_ElectronAffinity  entropy_ElectronAffinity  wtd_entropy_ElectronAffinity  range_ElectronAffinity  wtd_range_ElectronAffinity  std_ElectronAffinity  wtd_std_ElectronAffinity  mean_FusionHeat  wtd_mean_FusionHeat  gmean_FusionHeat  wtd_gmean_FusionHeat  entropy_FusionHeat  wtd_entropy_FusionHeat  range_FusionHeat  wtd_range_FusionHeat  std_FusionHeat  wtd_std_FusionHeat  mean_ThermalConductivity  wtd_mean_ThermalConductivity  gmean_ThermalConductivity  wtd_gmean_ThermalConductivity  entropy_ThermalConductivity  wtd_entropy_ThermalConductivity  range_ThermalConductivity  wtd_range_ThermalConductivity  std_ThermalConductivity  wtd_std_ThermalConductivity  mean_Valence  wtd_mean_Valence  gmean_Valence  wtd_gmean_Valence  entropy_Valence  wtd_entropy_Valence  range_Valence  wtd_range_Valence  std_Valence  wtd_std_Valence  critical_temp\n",
       "0                   4         88.944468             57.862692          66.361592              36.116612             1.181795                 1.062396          122.90607              31.794921        51.968828            53.622535   775.425   1010.268571  718.152900     938.016780     1.305967         0.791488      810.6     735.985714  323.811808   355.562967              160.25              105.514286           136.126003                84.528423               1.259244                   1.207040                  205                42.914286          75.237540              69.235569    4654.35725       2961.502286     724.953211          53.543811         1.033129             0.814598       8958.571        1579.583429  3306.162897      3572.596624                81.8375                 111.727143               60.123179                   99.414682                  1.159687                      0.787382                  127.05                   80.987143             51.433712                 42.558396           6.9055             3.846857          3.479475              1.040986            1.088575                0.994998            12.878              1.744571        4.599064            4.666920                107.756645                     61.015189                   7.062488                       0.621979                     0.308148                         0.262848                  399.97342                      57.127669               168.854244                   138.517163          2.25          2.257143       2.213364           2.219783         1.368922             1.066221              1           1.085714     0.433013         0.437059           29.0\n",
       "1                   5         92.729214             58.518416          73.132787              36.396602             1.449309                 1.057755          122.90607              36.161939        47.094633            53.979870   766.440   1010.612857  720.605511     938.745413     1.544145         0.807078      810.6     743.164286  290.183029   354.963511              161.20              104.971429           141.465215                84.370167               1.508328                   1.204115                  205                50.571429          67.321319              68.008817    5821.48580       3021.016571    1237.095080          54.095718         1.314442             0.914802      10488.571        1667.383429  3767.403176      3632.649185                90.8900                 112.316429               69.833315                  101.166398                  1.427997                      0.838666                  127.05                   81.207857             49.438167                 41.667621           7.7844             3.796857          4.403790              1.035251            1.374977                1.073094            12.878              1.595714        4.473363            4.603000                172.205316                     61.372331                  16.064228                       0.619735                     0.847404                         0.567706                  429.97342                      51.413383               198.554600                   139.630922          2.00          2.257143       1.888175           2.210679         1.557113             1.047221              2           1.128571     0.632456         0.468606           26.0\n",
       "2                   4         88.944468             57.885242          66.361592              36.122509             1.181795                 0.975980          122.90607              35.741099        51.968828            53.656268   775.425   1010.820000  718.152900     939.009036     1.305967         0.773620      810.6     743.164286  323.811808   354.804183              160.25              104.685714           136.126003                84.214573               1.259244                   1.132547                  205                49.314286          75.237540              67.797712    4654.35725       2999.159429     724.953211          53.974022         1.033129             0.760305       8958.571        1667.383429  3306.162897      3592.019281                81.8375                 112.213571               60.123179                  101.082152                  1.159687                      0.786007                  127.05                   81.207857             51.433712                 41.639878           6.9055             3.822571          3.479475              1.037439            1.088575                0.927479            12.878              1.757143        4.599064            4.649635                107.756645                     60.943760                   7.062488                       0.619095                     0.308148                         0.250477                  399.97342                      57.127669               168.854244                   138.540613          2.25          2.271429       2.213364           2.232679         1.368922             1.029175              1           1.114286     0.433013         0.444697           19.0\n",
       "3                   4         88.944468             57.873967          66.361592              36.119560             1.181795                 1.022291          122.90607              33.768010        51.968828            53.639405   775.425   1010.544286  718.152900     938.512777     1.305967         0.783207      810.6     739.575000  323.811808   355.183884              160.25              105.100000           136.126003                84.371352               1.259244                   1.173033                  205                46.114286          75.237540              68.521665    4654.35725       2980.330857     724.953211          53.758486         1.033129             0.788889       8958.571        1623.483429  3306.162897      3582.370597                81.8375                 111.970357               60.123179                  100.244950                  1.159687                      0.786900                  127.05                   81.097500             51.433712                 42.102344           6.9055             3.834714          3.479475              1.039211            1.088575                0.964031            12.878              1.744571        4.599064            4.658301                107.756645                     60.979474                   7.062488                       0.620535                     0.308148                         0.257045                  399.97342                      57.127669               168.854244                   138.528893          2.25          2.264286       2.213364           2.226222         1.368922             1.048834              1           1.100000     0.433013         0.440952           22.0\n",
       "4                   4         88.944468             57.840143          66.361592              36.110716             1.181795                 1.129224          122.90607              27.848743        51.968828            53.588771   775.425   1009.717143  718.152900     937.025573     1.305967         0.805230      810.6     728.807143  323.811808   356.319281              160.25              106.342857           136.126003                84.843442               1.259244                   1.261194                  205                36.514286          75.237540              70.634448    4654.35725       2923.845143     724.953211          53.117029         1.033129             0.859811       8958.571        1491.783429  3306.162897      3552.668664                81.8375                 111.240714               60.123179                   97.774719                  1.159687                      0.787396                  127.05                   80.766429             51.433712                 43.452059           6.9055             3.871143          3.479475              1.044545            1.088575                1.044970            12.878              1.744571        4.599064            4.684014                107.756645                     61.086617                   7.062488                       0.624878                     0.308148                         0.272820                  399.97342                      57.127669               168.854244                   138.493671          2.25          2.242857       2.213364           2.206963         1.368922             1.096052              1           1.057143     0.433013         0.428809           23.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"superconduct/train.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.3. Pré-processamento dos Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verificação da existência de valores nulos no conjunto de dados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.isnull(df).sum().max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.0. Objetivo 1 - Identificação das features mais importantes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separação do conjunto de dados em features (X) e target (y) e foi definida uma função que avalia os modelos RandomForestRegressor, DecisionTreeRegressor, LinearRegression e GradientBoostingRegressor (problema de regressão pelo facto de a target class não ser discreta mas sim contínua) de acordo com a variância explicada, de maneira a medir a discrepância entre cada um destes modelos e os dados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RVE RFs:  0.9180\n",
      "RVE DTs:  0.8743\n",
      "RVE LRs:  0.7374\n",
      "RVE GBs:  0.8601\n"
     ]
    }
   ],
   "source": [
    "df_array = np.array(df)\n",
    "col_names = np.array(df.columns)\n",
    "\n",
    "X = df_array[:, :-1]\n",
    "y = df_array[:, -1]\n",
    "\n",
    "features = col_names[:-1]\n",
    "target = col_names[-1]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=0)\n",
    "N,M = X_train.shape #N = number of samples, M = number of columns\n",
    "\n",
    "def modelTesting(X_train, X_test, y_train, y_test):\n",
    "    rfr = RandomForestRegressor(n_jobs=-1,random_state=0) #default n_estimators: 100\n",
    "    rfr.fit(X_train, y_train)\n",
    "\n",
    "    dtr = DecisionTreeRegressor(random_state=0)\n",
    "    dtr.fit(X_train, y_train)\n",
    "\n",
    "    lmr = LinearRegression()\n",
    "    lmr.fit(X_train, y_train)\n",
    "    \n",
    "    gbr=GradientBoostingRegressor(random_state=0)\n",
    "    gbr.fit(X_train, y_train)\n",
    "    \n",
    "    rf_preds=rfr.predict(X_test)\n",
    "    dt_preds=dtr.predict(X_test)\n",
    "    lr_preds=lmr.predict(X_test)\n",
    "    gb_preds=gbr.predict(X_test)\n",
    "\n",
    "    vrf=explained_variance_score(y_test, rf_preds)\n",
    "    vdt=explained_variance_score(y_test, dt_preds)\n",
    "    vlr=explained_variance_score(y_test, lr_preds)\n",
    "    vgb=explained_variance_score(y_test, gb_preds)\n",
    "    return vrf,vdt,vlr,vgb\n",
    "\n",
    "rf,dt,lr,gb=modelTesting(X_train, X_test, y_train, y_test)\n",
    "print(\"RVE RFs: %7.4f\" % rf)\n",
    "print(\"RVE DTs: %7.4f\" % dt)\n",
    "print(\"RVE LRs: %7.4f\" % lr)\n",
    "print(\"RVE GBs: %7.4f\" % gb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para a seleção das features mais importantes obtou-se pela utilização do modelo Random Forest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ordenação das features por ordem descendente da sua importância:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>indexs</th>\n",
       "      <th>importances</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>67</td>\n",
       "      <td>0.537363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>64</td>\n",
       "      <td>0.126767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9</td>\n",
       "      <td>0.026802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>80</td>\n",
       "      <td>0.020792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>74</td>\n",
       "      <td>0.015610</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   indexs  importances\n",
       "0      67     0.537363\n",
       "1      64     0.126767\n",
       "2       9     0.026802\n",
       "3      80     0.020792\n",
       "4      74     0.015610"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfr = RandomForestRegressor(random_state=0, n_jobs=-1)\n",
    "rfr.fit(X_train, y_train)\n",
    "\n",
    "feature_importances={'indexs':[],'importances':[]}\n",
    "for index, i in enumerate(rfr.feature_importances_):\n",
    "    feature_importances['indexs'].append(index)\n",
    "    feature_importances['importances'].append(i)\n",
    "importances_df=pd.DataFrame(feature_importances)\n",
    "importances_df=importances_df.sort_values(by=['importances'],ascending=False).reset_index(drop = True)\n",
    "importances_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De forma a podermos visualizar o impacto das features, construímos um gráfico para avaliar o impacto de cada feature nos resultados dos modelos RandomForestRegressor, DecisionTreeRegressor, LinearRegression e GradientBoostingRegressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = importances_df['importances'][:2]\n",
    "type(a)\n",
    "rf_results=list()\n",
    "dt_results=list()\n",
    "lr_results=list()\n",
    "gb_results=list()\n",
    "for i in range(1,len(importances_df['indexs'])+1):\n",
    "    best_cols=importances_df['indexs'][:i].tolist()\n",
    "    res=modelTesting(X_train[:,best_cols], X_test[:,best_cols], y_train, y_test)\n",
    "    rf_results.append(res[0])\n",
    "    dt_results.append(res[1])\n",
    "    lr_results.append(res[2])\n",
    "    gb_results.append(res[3])\n",
    "    #print(importances_df['indexs'][:i].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1224x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(figsize=[17,4])\n",
    "ax.plot(rf_results, color = 'green', label = 'Random Forest')\n",
    "ax.plot(dt_results, color = 'red', label = 'Decision Trees')\n",
    "ax.plot(lr_results, color = 'blue', label = 'Linear Regression')\n",
    "ax.plot(gb_results, color = 'black', label = 'Gradient Boosting')\n",
    "ax.legend(loc = 'lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No gráfico podemos observar que no intervalo entre o 15 e o 20 não é percetível nenhuma variação no resultado do modelo RandomForestRegressor, que é o modelo com melhores resultados. Apesar de termos dado maior importância ao RandomForestRegressor, também verificamos que não havia grandes variações nos resultados dos outros modelos.\n",
    "\n",
    "O número de variáveis escolhido foi de 18. Agora obtemos os resultados dos modelos com utilizando estas 18 features e imprimimos os seus indíces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Features:\n",
      " [67, 64, 9, 80, 74, 66, 43, 31, 72, 50, 70, 10, 35, 27, 36, 2, 18, 8] \n",
      "\n",
      "RVE RFs:  0.9137\n",
      "RVE DTs:  0.8563\n",
      "RVE LRs:  0.6435\n",
      "RVE GBs:  0.8502\n"
     ]
    }
   ],
   "source": [
    "nX_train=X_train[:,importances_df['indexs'][:i].tolist()[:18]]\n",
    "nX_test=X_test[:,importances_df['indexs'][:i].tolist()[:18]]\n",
    "rf,dt,lr,gb=modelTesting(nX_train, nX_test, y_train, y_test)\n",
    "print(\"Selected Features:\\n\", importances_df['indexs'][:i].tolist()[:18], \"\\n\")\n",
    "print(\"RVE RFs: %7.4f\" % rf)\n",
    "print(\"RVE DTs: %7.4f\" % dt)\n",
    "print(\"RVE LRs: %7.4f\" % lr)\n",
    "print(\"RVE GBs: %7.4f\" % gb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pelos resultados obtidos, utilizando apenas 1 features, é possível verificar que os valores continuam semelhantes aos obtidos com 82 features, sendo a maior diferença no modelo linear passando dos 0.7374 para 0.6435 e a menor diferença na RandomForest com o valor a passar dos 0.9180 para os 00.9136\n",
    "\n",
    "Devido a estas pequenas diferenças na variância explicada é possível uma redução acentuada no número de features do conjunto de dados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.0. Objetivo 2 - Redução de dimensionalidade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Devido à grande variedade de escalas dos valores existentes em cada features foi feita de seguida a estandardização dos dados de treino e teste separadamente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora escolhemos o número de componentes que vamos utilizar, para tal decidimos escolher o número de componentes a partir das quais obtivéssemos 0.95 da variância explicada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    }
   ],
   "source": [
    "pca = PCA(n_components=len(features))\n",
    "pca.fit(X_train_scaled)\n",
    "evr_sum=list()\n",
    "components95=None\n",
    "for index,value in enumerate(pca.explained_variance_ratio_):\n",
    "    if index==0:\n",
    "        evr_sum.append(value)\n",
    "    else:\n",
    "        if value+evr_sum[-1]>=0.95 and components95==None:\n",
    "            components95=index\n",
    "            print(index)\n",
    "        evr_sum.append(value+evr_sum[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RVE RFs:  0.9065\n",
      "RVE DTs:  0.8426\n",
      "RVE LRs:  0.5945\n",
      "RVE GBs:  0.8023\n"
     ]
    }
   ],
   "source": [
    "pca = PCA(n_components=components95)\n",
    "pca.fit(X_train_scaled)\n",
    "nX_train_scaled=pca.transform(X_train_scaled)\n",
    "nX_test_scaled=pca.transform(X_test_scaled)\n",
    "rf,dt,lr,gb=modelTesting(nX_train_scaled, nX_test_scaled, y_train, y_test)\n",
    "print(\"RVE RFs: %7.4f\" % rf)\n",
    "print(\"RVE DTs: %7.4f\" % dt)\n",
    "print(\"RVE LRs: %7.4f\" % lr)\n",
    "print(\"RVE GBs: %7.4f\" % gb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizando o PCA, a diferença de variância explicada em relação à obtida inicialmente com todas as features continua a não ser muita, apesar disso e em comparação com as obtidas utilizando a random forest os resultados obtidos pioraram em todos os modelos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.0. Objetivo 3 - Identificação do melhor modelo com as features anteriormente selecionadas:\n",
    "\n",
    "Como referido anteriormente, devido a tratar-se de um problema de regressão apenas foram utilizados modelos de regressão tais como: Linear Regression, Decision Tree Regressor, Random Forest Regressor, AdaBoost Regressor e por fim o Gradient Boosting Regressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "nX=X[:,importances_df['indexs'][:i].tolist()[:18]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De forma a avaliar os modelos foi criada uma função que de acordo com o modelo e os parâmetros dados calcula a variância explicada, assim como o RMSE do modelo utilizando o gridsearch com o parâmetro cv = 5, que retorna uma tabela com os resultados do modelo para diferentes valores dos parâmetros:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateModel(model, params, X_train, y_train, modelName, numberOfParams):\n",
    "    scoring = [\"explained_variance\", \"neg_root_mean_squared_error\"]\n",
    "    grid_cv = GridSearchCV(estimator= model, param_grid= params, scoring = scoring,cv=5, refit=\"explained_variance\", n_jobs=-1).fit(X_train, y_train)\n",
    "    results = pd.DataFrame(grid_cv.cv_results_)\n",
    "    #print(results)\n",
    "\n",
    "    if(numberOfParams == 2):\n",
    "        results = results.iloc[:, [4,5,6,12,-3]]\n",
    "    elif(numberOfParams == 0):\n",
    "        results = results.iloc[:, [4,10,-3]]\n",
    "    elif(numberOfParams == 3):\n",
    "        results = results.iloc[:, [4,5,6,7,13,-3]]\n",
    "\n",
    "    #add a column with the model Name\n",
    "    n = results.shape[0]\n",
    "    mn = []\n",
    "    for i in range(n):\n",
    "        mn.append(modelName)\n",
    "    results.insert(0, \"Model\", mn, True)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formatação das tabelas que serão construídas para guardar os scores de cada modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "pd.set_option('max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para este modelo apenas foi avaliado utilizando os valores default em todos os seus parâmetros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>params</th>\n",
       "      <th>mean_test_explained_variance</th>\n",
       "      <th>mean_test_neg_root_mean_squared_error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Linear Regression</td>\n",
       "      <td>{}</td>\n",
       "      <td>0.017732</td>\n",
       "      <td>-21.352081</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Model params  mean_test_explained_variance  mean_test_neg_root_mean_squared_error\n",
       "0  Linear Regression     {}                      0.017732                             -21.352081"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = [{}]\n",
    "\n",
    "nParams = len(params[0])\n",
    "\n",
    "linearResults = evaluateModel(LinearRegression(), params, nX, y, \"Linear Regression\", nParams)\n",
    "linearResults"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_min_samples_leaf</th>\n",
       "      <th>param_min_samples_split</th>\n",
       "      <th>params</th>\n",
       "      <th>mean_test_explained_variance</th>\n",
       "      <th>mean_test_neg_root_mean_squared_error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Decision Tree Regressor</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>{'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 2}</td>\n",
       "      <td>0.581616</td>\n",
       "      <td>-16.221866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Decision Tree Regressor</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>{'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 5}</td>\n",
       "      <td>0.583488</td>\n",
       "      <td>-16.227257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Decision Tree Regressor</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>{'max_depth': None, 'min_samples_leaf': 2, 'min_samples_split': 2}</td>\n",
       "      <td>0.580166</td>\n",
       "      <td>-16.086964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Decision Tree Regressor</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>{'max_depth': None, 'min_samples_leaf': 2, 'min_samples_split': 5}</td>\n",
       "      <td>0.581770</td>\n",
       "      <td>-16.166240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Decision Tree Regressor</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>{'max_depth': 3, 'min_samples_leaf': 1, 'min_samples_split': 2}</td>\n",
       "      <td>0.416027</td>\n",
       "      <td>-18.720723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Decision Tree Regressor</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>{'max_depth': 3, 'min_samples_leaf': 1, 'min_samples_split': 5}</td>\n",
       "      <td>0.416027</td>\n",
       "      <td>-18.720723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Decision Tree Regressor</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>{'max_depth': 3, 'min_samples_leaf': 2, 'min_samples_split': 2}</td>\n",
       "      <td>0.416027</td>\n",
       "      <td>-18.720723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Decision Tree Regressor</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>{'max_depth': 3, 'min_samples_leaf': 2, 'min_samples_split': 5}</td>\n",
       "      <td>0.416027</td>\n",
       "      <td>-18.720723</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Model param_max_depth param_min_samples_leaf param_min_samples_split                                                              params  mean_test_explained_variance  mean_test_neg_root_mean_squared_error\n",
       "0  Decision Tree Regressor            None                      1                       2  {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 2}                      0.581616                             -16.221866\n",
       "1  Decision Tree Regressor            None                      1                       5  {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 5}                      0.583488                             -16.227257\n",
       "2  Decision Tree Regressor            None                      2                       2  {'max_depth': None, 'min_samples_leaf': 2, 'min_samples_split': 2}                      0.580166                             -16.086964\n",
       "3  Decision Tree Regressor            None                      2                       5  {'max_depth': None, 'min_samples_leaf': 2, 'min_samples_split': 5}                      0.581770                             -16.166240\n",
       "4  Decision Tree Regressor               3                      1                       2     {'max_depth': 3, 'min_samples_leaf': 1, 'min_samples_split': 2}                      0.416027                             -18.720723\n",
       "5  Decision Tree Regressor               3                      1                       5     {'max_depth': 3, 'min_samples_leaf': 1, 'min_samples_split': 5}                      0.416027                             -18.720723\n",
       "6  Decision Tree Regressor               3                      2                       2     {'max_depth': 3, 'min_samples_leaf': 2, 'min_samples_split': 2}                      0.416027                             -18.720723\n",
       "7  Decision Tree Regressor               3                      2                       5     {'max_depth': 3, 'min_samples_leaf': 2, 'min_samples_split': 5}                      0.416027                             -18.720723"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = [{\"max_depth\": [None, 3],\n",
    "           \"min_samples_split\": [2, 5],\n",
    "           \"min_samples_leaf\": [1, 2]\n",
    "}]\n",
    "\n",
    "nParams = len(params[0])\n",
    "\n",
    "decisionTreeResults = evaluateModel(DecisionTreeRegressor(random_state=0), params, nX, y, \"Decision Tree Regressor\", nParams)\n",
    "decisionTreeResults"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>params</th>\n",
       "      <th>mean_test_explained_variance</th>\n",
       "      <th>mean_test_neg_root_mean_squared_error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Random Forest Regressor</td>\n",
       "      <td>None</td>\n",
       "      <td>100</td>\n",
       "      <td>{'max_depth': None, 'n_estimators': 100}</td>\n",
       "      <td>0.735419</td>\n",
       "      <td>-12.882849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Random Forest Regressor</td>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>{'max_depth': 3, 'n_estimators': 100}</td>\n",
       "      <td>0.435398</td>\n",
       "      <td>-18.409015</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Model param_max_depth param_n_estimators                                    params  mean_test_explained_variance  mean_test_neg_root_mean_squared_error\n",
       "0  Random Forest Regressor            None                100  {'max_depth': None, 'n_estimators': 100}                      0.735419                             -12.882849\n",
       "1  Random Forest Regressor               3                100     {'max_depth': 3, 'n_estimators': 100}                      0.435398                             -18.409015"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = [{\"n_estimators\": [100],\n",
    "           \"max_depth\": [None, 3]\n",
    "}]\n",
    "\n",
    "nParams = len(params[0])\n",
    "\n",
    "randomForestResults = evaluateModel(RandomForestRegressor(random_state=0), params, nX, y, \"Random Forest Regressor\", nParams)\n",
    "randomForestResults"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>param_base_estimator</th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>params</th>\n",
       "      <th>mean_test_explained_variance</th>\n",
       "      <th>mean_test_neg_root_mean_squared_error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AdaBoost Regressor</td>\n",
       "      <td>DecisionTreeRegressor()</td>\n",
       "      <td>50</td>\n",
       "      <td>{'base_estimator': DecisionTreeRegressor(), 'n_estimators': 50}</td>\n",
       "      <td>0.723493</td>\n",
       "      <td>-13.315493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AdaBoost Regressor</td>\n",
       "      <td>DecisionTreeRegressor()</td>\n",
       "      <td>100</td>\n",
       "      <td>{'base_estimator': DecisionTreeRegressor(), 'n_estimators': 100}</td>\n",
       "      <td>0.730651</td>\n",
       "      <td>-13.072578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AdaBoost Regressor</td>\n",
       "      <td>LinearRegression()</td>\n",
       "      <td>50</td>\n",
       "      <td>{'base_estimator': LinearRegression(), 'n_estimators': 50}</td>\n",
       "      <td>-0.069119</td>\n",
       "      <td>-22.010404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AdaBoost Regressor</td>\n",
       "      <td>LinearRegression()</td>\n",
       "      <td>100</td>\n",
       "      <td>{'base_estimator': LinearRegression(), 'n_estimators': 100}</td>\n",
       "      <td>-0.069119</td>\n",
       "      <td>-22.010404</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Model     param_base_estimator param_n_estimators                                                            params  mean_test_explained_variance  mean_test_neg_root_mean_squared_error\n",
       "0  AdaBoost Regressor  DecisionTreeRegressor()                 50   {'base_estimator': DecisionTreeRegressor(), 'n_estimators': 50}                      0.723493                             -13.315493\n",
       "1  AdaBoost Regressor  DecisionTreeRegressor()                100  {'base_estimator': DecisionTreeRegressor(), 'n_estimators': 100}                      0.730651                             -13.072578\n",
       "2  AdaBoost Regressor       LinearRegression()                 50        {'base_estimator': LinearRegression(), 'n_estimators': 50}                     -0.069119                             -22.010404\n",
       "3  AdaBoost Regressor       LinearRegression()                100       {'base_estimator': LinearRegression(), 'n_estimators': 100}                     -0.069119                             -22.010404"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = [{\"base_estimator\": [DecisionTreeRegressor(), LinearRegression()],\n",
    "    \"n_estimators\": [50, 100]\n",
    "}]\n",
    "\n",
    "nParams = len(params[0])\n",
    "\n",
    "adaBoostResults = evaluateModel(AdaBoostRegressor(random_state=0), params, nX, y, \"AdaBoost Regressor\", nParams)\n",
    "adaBoostResults"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5. Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>param_min_samples_split</th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>params</th>\n",
       "      <th>mean_test_explained_variance</th>\n",
       "      <th>mean_test_neg_root_mean_squared_error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Gradient Boosting Regressor</td>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>{'min_samples_split': 2, 'n_estimators': 100}</td>\n",
       "      <td>0.679806</td>\n",
       "      <td>-13.904874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Gradient Boosting Regressor</td>\n",
       "      <td>2</td>\n",
       "      <td>1000</td>\n",
       "      <td>{'min_samples_split': 2, 'n_estimators': 1000}</td>\n",
       "      <td>0.701975</td>\n",
       "      <td>-13.430961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Gradient Boosting Regressor</td>\n",
       "      <td>5</td>\n",
       "      <td>100</td>\n",
       "      <td>{'min_samples_split': 5, 'n_estimators': 100}</td>\n",
       "      <td>0.679131</td>\n",
       "      <td>-13.918736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Gradient Boosting Regressor</td>\n",
       "      <td>5</td>\n",
       "      <td>1000</td>\n",
       "      <td>{'min_samples_split': 5, 'n_estimators': 1000}</td>\n",
       "      <td>0.699378</td>\n",
       "      <td>-13.494588</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Model param_min_samples_split param_n_estimators                                          params  mean_test_explained_variance  mean_test_neg_root_mean_squared_error\n",
       "0  Gradient Boosting Regressor                       2                100   {'min_samples_split': 2, 'n_estimators': 100}                      0.679806                             -13.904874\n",
       "1  Gradient Boosting Regressor                       2               1000  {'min_samples_split': 2, 'n_estimators': 1000}                      0.701975                             -13.430961\n",
       "2  Gradient Boosting Regressor                       5                100   {'min_samples_split': 5, 'n_estimators': 100}                      0.679131                             -13.918736\n",
       "3  Gradient Boosting Regressor                       5               1000  {'min_samples_split': 5, 'n_estimators': 1000}                      0.699378                             -13.494588"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = [{\"n_estimators\": [100, 1000],\n",
    "           \"min_samples_split\": [2, 5]\n",
    "}]\n",
    "\n",
    "nParams = len(params[0])\n",
    "\n",
    "gradientBoostingResults = evaluateModel(GradientBoostingRegressor(random_state=0), params, nX, y, \"Gradient Boosting Regressor\", nParams)\n",
    "gradientBoostingResults"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6. Resultados finais"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concluída a tabulação dos scores para cada modelo, passou-se para a concatenação dessas mesmas tabelas de forma a obtermos os resultados de todos os modelos numa só tabela:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "allResults = pd.concat([linearResults, decisionTreeResults, adaBoostResults, gradientBoostingResults, randomForestResults])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7. Top 10 de melhores modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por fim seram selecionados os dez melhores modelos com base na variância explicada obtida em cada um dos modelos treinados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>params</th>\n",
       "      <th>mean_test_explained_variance</th>\n",
       "      <th>mean_test_neg_root_mean_squared_error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Random Forest Regressor</td>\n",
       "      <td>{'max_depth': None, 'n_estimators': 100}</td>\n",
       "      <td>0.735419</td>\n",
       "      <td>-12.882849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AdaBoost Regressor</td>\n",
       "      <td>{'base_estimator': DecisionTreeRegressor(), 'n_estimators': 100}</td>\n",
       "      <td>0.730651</td>\n",
       "      <td>-13.072578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AdaBoost Regressor</td>\n",
       "      <td>{'base_estimator': DecisionTreeRegressor(), 'n_estimators': 50}</td>\n",
       "      <td>0.723493</td>\n",
       "      <td>-13.315493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Gradient Boosting Regressor</td>\n",
       "      <td>{'min_samples_split': 2, 'n_estimators': 1000}</td>\n",
       "      <td>0.701975</td>\n",
       "      <td>-13.430961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Gradient Boosting Regressor</td>\n",
       "      <td>{'min_samples_split': 5, 'n_estimators': 1000}</td>\n",
       "      <td>0.699378</td>\n",
       "      <td>-13.494588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Gradient Boosting Regressor</td>\n",
       "      <td>{'min_samples_split': 2, 'n_estimators': 100}</td>\n",
       "      <td>0.679806</td>\n",
       "      <td>-13.904874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Gradient Boosting Regressor</td>\n",
       "      <td>{'min_samples_split': 5, 'n_estimators': 100}</td>\n",
       "      <td>0.679131</td>\n",
       "      <td>-13.918736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Decision Tree Regressor</td>\n",
       "      <td>{'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 5}</td>\n",
       "      <td>0.583488</td>\n",
       "      <td>-16.227257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Decision Tree Regressor</td>\n",
       "      <td>{'max_depth': None, 'min_samples_leaf': 2, 'min_samples_split': 5}</td>\n",
       "      <td>0.581770</td>\n",
       "      <td>-16.166240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Decision Tree Regressor</td>\n",
       "      <td>{'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 2}</td>\n",
       "      <td>0.581616</td>\n",
       "      <td>-16.221866</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Model                                                              params  mean_test_explained_variance  mean_test_neg_root_mean_squared_error\n",
       "0      Random Forest Regressor                            {'max_depth': None, 'n_estimators': 100}                      0.735419                             -12.882849\n",
       "1           AdaBoost Regressor    {'base_estimator': DecisionTreeRegressor(), 'n_estimators': 100}                      0.730651                             -13.072578\n",
       "2           AdaBoost Regressor     {'base_estimator': DecisionTreeRegressor(), 'n_estimators': 50}                      0.723493                             -13.315493\n",
       "3  Gradient Boosting Regressor                      {'min_samples_split': 2, 'n_estimators': 1000}                      0.701975                             -13.430961\n",
       "4  Gradient Boosting Regressor                      {'min_samples_split': 5, 'n_estimators': 1000}                      0.699378                             -13.494588\n",
       "5  Gradient Boosting Regressor                       {'min_samples_split': 2, 'n_estimators': 100}                      0.679806                             -13.904874\n",
       "6  Gradient Boosting Regressor                       {'min_samples_split': 5, 'n_estimators': 100}                      0.679131                             -13.918736\n",
       "7      Decision Tree Regressor  {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 5}                      0.583488                             -16.227257\n",
       "8      Decision Tree Regressor  {'max_depth': None, 'min_samples_leaf': 2, 'min_samples_split': 5}                      0.581770                             -16.166240\n",
       "9      Decision Tree Regressor  {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 2}                      0.581616                             -16.221866"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allResults = allResults.iloc[:, :4]\n",
    "top10 = allResults.nlargest(10, \"mean_test_explained_variance\").reset_index(drop = True)\n",
    "top10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se pode observar na tabela de cima, o modelo Random Forest Regressor destaca-se como o melhor modelo treinado chegando aos 74% de variância explicada utilizando os parâmetros em default.\n",
    "\n",
    "Destaque para o facto de o modelo Linear Regression não aparecer nos dez melhores modelos.\n",
    "\n",
    "É possível observar que o Gradient Boosting Regressor, apresenta melhores resultados quando o modelo tem parâmetros que não estão em default. "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
